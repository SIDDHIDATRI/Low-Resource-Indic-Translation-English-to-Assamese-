This project focuses on developing a low-resource neural machine translation (NMT) system for the English–Assamese language pair using a pretrained multilingual Transformer architecture. Given the scarcity of large-scale parallel corpora for Assamese, the study adopts a parameter-efficient fine-tuning approach based on Low-Rank Adaptation (LoRA) to adapt the existing facebook/nllb-200-distilled-600M model for the specific translation task.
LoRA fine-tuning introduces a small number of trainable parameters into selected layers of the Transformer network, enabling the model to learn language-specific patterns without requiring complete retraining. This makes the method highly efficient in terms of computational cost and resource utilisation.
The fine-tuning process is performed on a limited English–Assamese parallel dataset, using a sequence-to-sequence Transformer framework for both training and inference. Evaluation of the adapted model with SacreBLEU scores shows that the LoRA-based adaptation effectively transfers cross-lingual knowledge from related Indic languages to Assamese, leading to improved	translation	fluency	and	adequacy. Overall, this study demonstrates that LoRA fine-tuning is a practical and scalable approach for enhancing translation performance in low-resource Indic languages, supporting the development of inclusive and resource-efficient machine translation research.
